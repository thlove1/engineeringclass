{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawlvsscrap 2022-10-27 20-10-00.201336.txt로 저장됩니다...\n",
      "과도한 금리 정보수집에 저축은행 '몸살'…\"스크래핑 막는다\"\n",
      "하이픈, 데이터마켓에서 인포텍 세무신고서비스 스크래핑 패키지 솔루션 공급\n",
      "카뱅, 개인사업자 시장 출격…\"당연히 누려야 했던 '편리함' 돌려드릴 것\"\n",
      "카뱅, '527만 데이터' 개인사업자 시장 공략…내달 1일 신용대출 출시\n",
      "카카오뱅크, 개인사업자 '틈새시장' 공략…\"새로운 뱅킹 경험 선사\"(영상)\n",
      "하이픈, 인포텍 ‘세부신고 서비스 스크래핑 패키지 솔루션’ 공급\n",
      "저축은행 전산망 과부하 먹통 우려\n",
      "토스, 결제 정보 '스크래핑'…데이터 독점 우려\n",
      "스크래핑 논란 진화 나선 토스 자회사…가맹점관리 서비스 중단\n",
      "에버스핀, '카드사 무단 스크래핑' 방지 서비스 공급 나서\n",
      "형사는 무죄, 민사는 “10억 배상”…데이터 크롤링 어디까지 되나\n",
      "현대카드, ‘2022 소비자 패널 간담회’ 개최\n",
      "해시스크래퍼, 2022 한국산업대전 참가\n",
      "[2022 로보월드 특집] 연구교육기관\n",
      "클릭 한 번으로 할 수 있는 업계별 데이터 수집 활용법: 데이터 크롤링 10만 유저를 분석하다\n",
      "[2022 국감] 위법성 도마 위 오른 삼쩜삼…개인정보 다 빨린다\n",
      "정태영 현대카드 부회장, 소비자보호에 ‘테크’ 역량 더해 시스템 체계화 [금융소비자보호 진단 ③]\n",
      "[대한민국 인공지능 기업정보집] AI 메타데이터로 VOD 추천 솔루션 개발한 ‘오투오’\n",
      "스토킹 피해 호소 10명 중 1명은 미성년자\n",
      "‘야놀자vs여기어때’ 크롤링 소송…법무법인 민후의 연이은 승소 비결은?\n"
     ]
    }
   ],
   "source": [
    "#@title 크롤링 vs 스크래핑 네이버 빈도검색!\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "d = str(datetime.datetime.now())\n",
    "dm = d.replace(\":\", \"-\")\n",
    "crawlnum = 0\n",
    "scrapnum = 0\n",
    "urlaboutcrawl = \"https://search.naver.com/search.naver?sm=tab_hty.top&where=news&query=%ED%81%AC%EB%A1%A4%EB%A7%81+-%EC%8A%A4%ED%81%AC%EB%9E%98%ED%95%91&oquery=%EC%8A%A4%ED%81%AC%EB%9E%98%ED%95%91+-%ED%81%AC%EB%A1%A4%EB%A7%81&tqi=h1i9jsprvhGssnvVeLVssssss58-292984&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&mynews=0&office_section_code=0&office_type=0&pd=0&photo=0&sort=0\"\n",
    "urlaboutscrap = \"https://search.naver.com/search.naver?sm=tab_hty.top&where=news&query=%EC%8A%A4%ED%81%AC%EB%9E%98%ED%95%91+-%ED%81%AC%EB%A1%A4%EB%A7%81&oquery=%ED%81%AC%EB%A1%A4%EB%A7%81+-%EC%8A%A4%ED%81%AC%EB%9E%98%ED%95%91&tqi=h1i8DlprvhGssn58zh0ssssstvo-456906&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&mynews=0&office_section_code=0&office_type=0&pd=0&photo=0&sort=0\"\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "response1 = urlopen(urlaboutscrap)\n",
    "soup1 = BeautifulSoup(response1, 'html.parser')\n",
    "    \n",
    "f = open(f\"crawlncloud\\crawlvsscrap {dm}.txt\", 'w+', encoding=\"utf-8\") # 파일 열기\n",
    "print(f\"crawlvsscrap {dm}.txt로 저장됩니다...\")\n",
    "f.write(\"스크래핑만 쓰인 뉴스 : \"+\"\\n\")\n",
    "\n",
    "for anchor in soup1.select(\"a.news_tit\"): #soup중에 a태그 안에 news_tit클래스를 찾아 찾아서 anchor에 담는다.\n",
    "  data = anchor.get('title') #anchor에서 title(뉴스제목)을 get해서 출력 또는 anchor.get_text()로 뽑을 수도 있다.\n",
    "  print(data)\n",
    "  f.write(data+\"\\n\")\n",
    "  scrapnum +=1\n",
    "\n",
    "response2 = urlopen(urlaboutcrawl)\n",
    "soup2 = BeautifulSoup(response2, 'html.parser')\n",
    "f.write(\"크롤링만 쓰인 뉴스 : \"+\"\\n\")\n",
    "for anchor in soup2.select(\"a.news_tit\"): #soup중에 a태그 안에 news_tit클래스를 찾아 찾아서 anchor에 담는다.\n",
    "  data = anchor.get('title') #anchor에서 title(뉴스제목)을 get해서 출력 또는 anchor.get_text()로 뽑을 수도 있다.\n",
    "  print(data)\n",
    "  f.write(data+\"\\n\")\n",
    "  crawlnum += 1\n",
    "f.write(f\"스크래핑만 쓰인 뉴스{scrapnum}개, 크롤링만 쓰인 뉴스{crawlnum}개가 검색되었습니다.\")    \n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
